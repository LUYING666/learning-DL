#### 一.过拟合、欠拟合及其解决方案
欠拟合（underfitting）：模型无法得到较低的训练误差；
过拟合（overfitting）：模型的训练误差远小于它在测试数据集上的误差。
解决欠拟合可以考虑增加模型复杂度。
解决过拟合：
 - 增加训练数据集的大小
 - L2范数正则化（又叫权重衰减）：在损失函数的基础上添加了L2范数惩罚项。L2范数惩罚项通过惩罚绝对值较大的模型参数为模型增加了限制。
 - 丢弃法：丢弃法通过随机丢弃层间元素，使模型不依赖于某一个元素来应对过拟合。
#### 二.梯度消失、梯度爆炸
当神经网络的层数较多时，模型的数值稳定性容易变差。梯度的计算也容易出现消失或者爆炸。
梯度消失会导致模型训练困难，对参数的优化步长过小，收效甚微，模型收敛十分缓慢。
梯度爆炸会导致模型训练困难，对参数的优化步长过大，难以收敛。
激活函数使用sigmoid或者tanh会把元素转换到[0, 1]和[-1, 1]之间，会加剧梯度消失的现象。
概念偏移可以根据其缓慢变化的特点缓解。
协变量偏移和标签偏移可能同时发生。
标签偏移可以简单理解为测试时出现了训练时没有的标签。
如果数据量足够的情况下，确保训练数据集和测试集中的数据取自同一个数据集，可以防止协变量偏移和标签偏移。
协变量偏移是x变 标签偏移是y变 概念偏移是y的定义变。

#### 三、循环神经网络进阶
待续

