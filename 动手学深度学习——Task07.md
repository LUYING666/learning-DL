#### 优化算法进阶
#### word2vec
基于两种概率模型的假设，我们可以定义两种 Word2Vec 模型：
1.Skip-Gram 跳字模型
2.CBOW (continuous bag-of-words) 连续词袋模型
#### 词嵌入进阶
我们在小规模数据集上训练了一个 Word2Vec 词嵌入模型，并通过词向量的余弦相似度搜索近义词。虽然 Word2Vec 已经能够成功地将离散的单词转换为连续的词向量，并能一定程度上地保存词与词之间的近似关系，但 Word2Vec 模型仍不是完美的，它还可以被进一步地改进：
子词嵌入（subword embedding）：FastText 以固定大小的 n-gram 形式将单词更细致地表示为了子词的集合，而 BPE (byte pair encoding) 算法则能根据语料库的统计信息，自动且动态地生成高频子词的集合；
GloVe 全局向量的词嵌入: 通过等价转换 Word2Vec 模型的条件概率公式，我们可以得到一个全局的损失函数表达，并在此基础上进一步优化模型。
